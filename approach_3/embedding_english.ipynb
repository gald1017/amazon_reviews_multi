{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5788021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "#from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6e6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok(tokenizer, text):\n",
    "    return tokenizer.encode_plus(text, \n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=510,\n",
    "                                    padding='longest', \n",
    "                                    truncation=True,\n",
    "                                      return_token_type_ids=True,\n",
    "                                      return_attention_mask=True,\n",
    "                                      return_tensors='pt'\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e44f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_embed(df, model, tokenizer):\n",
    "    l = []\n",
    "    for i, review in enumerate(df.review_body):\n",
    "        tokened = tok(tokenizer, review)\n",
    "        #print(model(**tokenizer))\n",
    "        l.append(embed(model_mul, tokened).numpy())\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dd3c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Tuple\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "import torch\n",
    "\n",
    "def embed(model, tokens_tensor ):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "        model.to('cuda')\n",
    "        outputs = model(**tokens_tensor)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        # Stores the token vectors, with shape [6 x 768]\n",
    "    \n",
    "    token_vecs_sum = []\n",
    "\n",
    "    # `token_embeddings` is a [6 x 12 x 768] tensor.\n",
    "\n",
    "    # For each token in the sentence...\n",
    "    for token in token_embeddings:\n",
    "\n",
    "        # `token` is a [6 x 768] tensor\n",
    "\n",
    "        # Sum the vectors from the last four layers.\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "        # Use `sum_vec` to represent `token`.\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "        \n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all 6 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    \n",
    "    return sentence_embedding\n",
    "\n",
    "def embed_cls(model, tokens_tensor):\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokens_tensor = tokens_tensor.to('cuda')       \n",
    "        model.to('cuda')\n",
    "        \n",
    "        #if torch.cuda.device_count() > 1:\n",
    "            #print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "            #model = nn.DataParallel(model)\n",
    "        #else:\n",
    "            #model.to(device)\n",
    "        \n",
    "        outputs = model(**tokens_tensor)\n",
    "        \n",
    "        return outputs.pooler_output\n",
    "        \n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "            self,\n",
    "            bert_tokenizer,\n",
    "            bert_model,\n",
    "            max_length: int = 510,\n",
    "            embedding_func = None,\n",
    "    ):\n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.model = bert_model\n",
    "        self.model.eval()\n",
    "        self.max_length = max_length\n",
    "        self.embedding_func = embedding_func\n",
    "\n",
    "        if self.embedding_func is None:\n",
    "            self.embedding_func = lambda x: x[0][:, 0, :].squeeze()\n",
    "\n",
    "    def _tokenize(self, text: str) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        # Tokenize the text with the provided tokenizer\n",
    "#         tokenized_text = self.tokenizer.encode_plus(text,\n",
    "#                                                     add_special_tokens=True,\n",
    "#                                                     max_length=self.max_length\n",
    "#                                                     )[\"input_ids\"]\n",
    "        \n",
    "        tokenized_text = self.tokenizer.encode_plus(text, \n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length=self.max_length,\n",
    "                                    padding='longest', \n",
    "                                    truncation=True,\n",
    "                                    return_token_type_ids=True,\n",
    "                                    return_attention_mask=True,\n",
    "                                    return_tensors='pt'\n",
    "                                    )\n",
    "        return tokenized_text\n",
    "\n",
    "\n",
    "    def _tokenize_and_predict(self, text: str) -> torch.tensor:\n",
    "        tokenized = self._tokenize(text)\n",
    "\n",
    "        #embeddings = self.model(**tokenized)\n",
    "        return self.embedding_func(self.model, tokenized)\n",
    "\n",
    "    def transform(self, text: List[str]):\n",
    "        if isinstance(text, pd.Series):\n",
    "            text = text.tolist()\n",
    "        \n",
    "#         return torch.stack([self._tokenize_and_predict(string) for string in text]).cpu()\n",
    "\n",
    "        return torch.stack([self._tokenize_and_predict(text)]).cpu()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb4e47",
   "metadata": {},
   "source": [
    "# Data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74839fc",
   "metadata": {},
   "source": [
    "### English dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08af2fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train = pd.read_csv('../data/train_en').review_body\n",
    "eng_test = pd.read_csv('../data/test_en').review_body\n",
    "eng_val = pd.read_csv('../data/val_en').review_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71efb20f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb21637611594986bd753de99bacc915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b895371a5ace47998df79883f9a5dc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef316028959c4f4bbe1b7cdb498547e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01dee4eae5a4f3aaa67a95e349f8bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "tokenizer_mul = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model_mul = RobertaModel.from_pretrained('roberta-base',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b6c5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_transformer = BertTransformer(tokenizer_mul, model_mul, embedding_func=embed)\n",
    "bert_transformer = BertTransformer(tokenizer_mul, model_mul, embedding_func=embed_cls)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a3b64bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.398080348968506\n",
      "40.467787742614746\n",
      "37.07446479797363\n",
      "37.14466166496277\n",
      "1515.7183821201324\n",
      "1519.1222438812256\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "\n",
    "texts = [eng_test, eng_val, eng_train ]\n",
    "files_name = ['eng_test_roberta_embedded.pkl', 'eng_val_roberta_embedded.pkl', 'eng_train_roberta_embedded.pkl' ]\n",
    "\n",
    "\n",
    "for i,t in enumerate(texts):\n",
    "    emebedded = []\n",
    "    start = time.time()\n",
    "    for k,com in enumerate(t):\n",
    "        emebedded.append(bert_transformer.transform(com))\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    \n",
    "    with open(files_name[i], 'wb') as f:\n",
    "        torch.save(list(itertools.chain(*emebedded)), f)    \n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c043955",
   "metadata": {},
   "source": [
    "## Spanish translated to English - embedding using roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27cf3568",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_to_eng_train = torch.load('amz_train_es_to_en_Translation_Helsinki.pkl')\n",
    "es_to_eng_test = torch.load('amz_val_es_to_en_Translation_Helsinki.pkl')\n",
    "es_to_eng_val = torch.load('amz_val_en_to_es_Translation_Helsinki.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02013e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.88653063774109\n",
      "37.96212816238403\n",
      "37.30016016960144\n",
      "37.37120580673218\n",
      "1488.6091227531433\n",
      "1492.014592409134\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "\n",
    "texts = [es_to_eng_val, es_to_eng_test, es_to_eng_train ]\n",
    "files_name = ['es_to_eng_val_embedded.pkl', 'es_to_eng_test_embedded.pkl', 'es_to_eng_train_embedded.pkl' ]\n",
    "\n",
    "\n",
    "for i,t in enumerate(texts):\n",
    "    emebedded = []\n",
    "    start = time.time()\n",
    "    for k,com in enumerate(t):\n",
    "        emebedded.append(bert_transformer.transform(com))\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    \n",
    "    with open(files_name[i], 'wb') as f:\n",
    "        torch.save(list(itertools.chain(*emebedded)), f)    \n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848747e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
